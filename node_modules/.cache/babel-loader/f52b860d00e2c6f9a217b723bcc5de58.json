{"ast":null,"code":"import React from\"react\";import Layout,{InlineMath,BlockMath,SyntaxHighlighter}from\"../../Layout\";function Algorithm(){return/*#__PURE__*/React.createElement(\"main\",null,/*#__PURE__*/React.createElement(\"hr\",null),/*#__PURE__*/React.createElement(\"h2\",null,\"Kmeans by SKlearn\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"c = \\\\\\\\pm\\\\\\\\sqrt{a^2 + b^2}\"}),/*#__PURE__*/React.createElement(SyntaxHighlighter,{language:\"python\"},\"import numpy as np\\n          from sklearn.cluster import KMeans\\n          X = np.array([[1, 2], [1, 4], [1, 0],\\n                        [10, 2], [10, 4], [10, 0]])\\n          kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\\n          print(kmeans.labels_)\\n          \"),/*#__PURE__*/React.createElement(\"hr\",null),/*#__PURE__*/React.createElement(\"h2\",null,\"Gradient descent\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"\\\\\\\\mathbf{x}_{\\\\\\\\text{new}} = \\\\\\\\mathbf{x} - \\\\\\\\eta \\\\\\\\nabla f(\\\\\\\\mathbf{x})\"}),/*#__PURE__*/React.createElement(SyntaxHighlighter,{language:\"python\"},\"import numpy as np\\n          def gradient_descent(x, grad, eta=0.01, n_iter=100):\\n            for _ in range(n_iter):\\n              x -= eta * grad(x)\\n            return x\\n          \"),/*#__PURE__*/React.createElement(\"hr\",null),/*#__PURE__*/React.createElement(\"h2\",null,\"Newton's Method for Optimization\"),/*#__PURE__*/React.createElement(\"p\",null,\"Newton's method, also known as the Newton-Raphson method, is an advanced optimization technique that uses second-order information to find the minima of a function more efficiently compared to first-order methods like gradient descent. This method not only considers the gradient (first derivatives) but also the curvature of the function via the Hessian (second derivatives), allowing for adaptive step sizes and faster convergence.\"),/*#__PURE__*/React.createElement(\"p\",null,\"Here\\u2019s how Newton's step works in the context of optimization:\",/*#__PURE__*/React.createElement(\"ol\",null,/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"strong\",null,\"Objective Function\"),\": You have a function you want to minimize.\"),/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"strong\",null,\"Gradient and Hessian\"),\": Compute the gradient and the Hessian matrix.\"),/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"strong\",null,\"Newton's Update Rule\"),\": Update the parameters using both the gradient and the inverse of the Hessian.\"),/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"strong\",null,\"Convergence\"),\": Repeat until the change is small, indicating proximity to a minimum.\"))),/*#__PURE__*/React.createElement(BlockMath,{math:\"\\\\\\\\mathbf{x}_{\\\\\\\\text{new}} = \\\\\\\\mathbf{x} - H(\\\\\\\\mathbf{x})^{-1} \\\\\\\\nabla f(\\\\\\\\mathbf{x})\"}),/*#__PURE__*/React.createElement(SyntaxHighlighter,{language:\"python\"},\"def newtons_method(f, df, ddf, x0, tol=1e-5, max_iter=50):\\n            x = x0\\n            for _ in range(max_iter):\\n                H_inv = np.linalg.inv(ddf(x))\\n                x_new = x - H_inv @ df(x)\\n                if np.linalg.norm(x_new - x) < tol:\\n                    break\\n                x = x_new\\n            return x\\n          \"),/*#__PURE__*/React.createElement(\"hr\",null),/*#__PURE__*/React.createElement(\"h2\",null,\"DarkQuest\"),/*#__PURE__*/React.createElement(SyntaxHighlighter,{language:\"cpp\"},\"#include <iostream>\\n          int main() {\\n            std::cout << \\\"Hello, World!\\\";\\n            return 0;\\n          }\\n          \"),/*#__PURE__*/React.createElement(\"hr\",null),/*#__PURE__*/React.createElement(\"h2\",null,\"Support Vector Machine by SKlearn\"),/*#__PURE__*/React.createElement(\"hr\",null));}export default Algorithm;","map":{"version":3,"names":["React","Layout","InlineMath","BlockMath","SyntaxHighlighter","Algorithm","createElement","math","language"],"sources":["/Users/wongdowling/Documents/my_notes/src/subsections/Algorithm/GradientDescent.js"],"sourcesContent":["import React from \"react\";\nimport Layout, { InlineMath, BlockMath, SyntaxHighlighter } from \"../../Layout\";\n\nfunction Algorithm() {\n  return (\n    <main>\n      <hr />\n      <h2>Kmeans by SKlearn</h2>\n      <BlockMath math=\"c = \\\\pm\\\\sqrt{a^2 + b^2}\" />\n      <SyntaxHighlighter language=\"python\">\n        {`import numpy as np\n          from sklearn.cluster import KMeans\n          X = np.array([[1, 2], [1, 4], [1, 0],\n                        [10, 2], [10, 4], [10, 0]])\n          kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n          print(kmeans.labels_)\n          `}\n      </SyntaxHighlighter>\n      <hr />\n      <h2>Gradient descent</h2>\n      <BlockMath math=\"\\\\mathbf{x}_{\\\\text{new}} = \\\\mathbf{x} - \\\\eta \\\\nabla f(\\\\mathbf{x})\" />\n      <SyntaxHighlighter language=\"python\">\n        {`import numpy as np\n          def gradient_descent(x, grad, eta=0.01, n_iter=100):\n            for _ in range(n_iter):\n              x -= eta * grad(x)\n            return x\n          `}\n      </SyntaxHighlighter>\n      <hr />\n      <h2>Newton's Method for Optimization</h2>\n      <p>\n        Newton's method, also known as the Newton-Raphson method, is an advanced optimization technique that uses second-order information to find the minima of a function more efficiently compared to first-order methods like gradient descent. This method not only considers the gradient (first derivatives) but also the curvature of the function via the Hessian (second derivatives), allowing for adaptive step sizes and faster convergence.\n      </p>\n      <p>\n        Hereâ€™s how Newton's step works in the context of optimization:\n        <ol>\n          <li><strong>Objective Function</strong>: You have a function you want to minimize.</li>\n          <li><strong>Gradient and Hessian</strong>: Compute the gradient and the Hessian matrix.</li>\n          <li><strong>Newton's Update Rule</strong>: Update the parameters using both the gradient and the inverse of the Hessian.</li>\n          <li><strong>Convergence</strong>: Repeat until the change is small, indicating proximity to a minimum.</li>\n        </ol>\n      </p>\n      <BlockMath math=\"\\\\mathbf{x}_{\\\\text{new}} = \\\\mathbf{x} - H(\\\\mathbf{x})^{-1} \\\\nabla f(\\\\mathbf{x})\" />\n      <SyntaxHighlighter language=\"python\">\n        {`def newtons_method(f, df, ddf, x0, tol=1e-5, max_iter=50):\n            x = x0\n            for _ in range(max_iter):\n                H_inv = np.linalg.inv(ddf(x))\n                x_new = x - H_inv @ df(x)\n                if np.linalg.norm(x_new - x) < tol:\n                    break\n                x = x_new\n            return x\n          `}\n      </SyntaxHighlighter>\n      <hr />\n      <h2>DarkQuest</h2>\n      <SyntaxHighlighter language=\"cpp\">\n        {`#include <iostream>\n          int main() {\n            std::cout << \"Hello, World!\";\n            return 0;\n          }\n          `}\n      </SyntaxHighlighter>\n      <hr />\n      <h2>Support Vector Machine by SKlearn</h2>\n      <hr />\n    </main>\n  );\n}\n\nexport default Algorithm;\n"],"mappings":"AAAA,MAAO,CAAAA,KAAK,KAAM,OAAO,CACzB,MAAO,CAAAC,MAAM,EAAIC,UAAU,CAAEC,SAAS,CAAEC,iBAAiB,KAAQ,cAAc,CAE/E,QAAS,CAAAC,SAASA,CAAA,CAAG,CACnB,mBACEL,KAAA,CAAAM,aAAA,0BACEN,KAAA,CAAAM,aAAA,UAAK,CAAC,cACNN,KAAA,CAAAM,aAAA,WAAI,mBAAqB,CAAC,cAC1BN,KAAA,CAAAM,aAAA,CAACH,SAAS,EAACI,IAAI,CAAC,+BAA2B,CAAE,CAAC,cAC9CP,KAAA,CAAAM,aAAA,CAACF,iBAAiB,EAACI,QAAQ,CAAC,QAAQ,uRAQjB,CAAC,cACpBR,KAAA,CAAAM,aAAA,UAAK,CAAC,cACNN,KAAA,CAAAM,aAAA,WAAI,kBAAoB,CAAC,cACzBN,KAAA,CAAAM,aAAA,CAACH,SAAS,EAACI,IAAI,CAAC,oFAAwE,CAAE,CAAC,cAC3FP,KAAA,CAAAM,aAAA,CAACF,iBAAiB,EAACI,QAAQ,CAAC,QAAQ,+LAOjB,CAAC,cACpBR,KAAA,CAAAM,aAAA,UAAK,CAAC,cACNN,KAAA,CAAAM,aAAA,WAAI,kCAAoC,CAAC,cACzCN,KAAA,CAAAM,aAAA,UAAG,mbAEA,CAAC,cACJN,KAAA,CAAAM,aAAA,UAAG,qEAED,cAAAN,KAAA,CAAAM,aAAA,wBACEN,KAAA,CAAAM,aAAA,wBAAIN,KAAA,CAAAM,aAAA,eAAQ,oBAA0B,CAAC,8CAA+C,CAAC,cACvFN,KAAA,CAAAM,aAAA,wBAAIN,KAAA,CAAAM,aAAA,eAAQ,sBAA4B,CAAC,iDAAkD,CAAC,cAC5FN,KAAA,CAAAM,aAAA,wBAAIN,KAAA,CAAAM,aAAA,eAAQ,sBAA4B,CAAC,kFAAmF,CAAC,cAC7HN,KAAA,CAAAM,aAAA,wBAAIN,KAAA,CAAAM,aAAA,eAAQ,aAAmB,CAAC,yEAA0E,CACxG,CACH,CAAC,cACJN,KAAA,CAAAM,aAAA,CAACH,SAAS,EAACI,IAAI,CAAC,kGAAsF,CAAE,CAAC,cACzGP,KAAA,CAAAM,aAAA,CAACF,iBAAiB,EAACI,QAAQ,CAAC,QAAQ,gWAWjB,CAAC,cACpBR,KAAA,CAAAM,aAAA,UAAK,CAAC,cACNN,KAAA,CAAAM,aAAA,WAAI,WAAa,CAAC,cAClBN,KAAA,CAAAM,aAAA,CAACF,iBAAiB,EAACI,QAAQ,CAAC,KAAK,4IAOd,CAAC,cACpBR,KAAA,CAAAM,aAAA,UAAK,CAAC,cACNN,KAAA,CAAAM,aAAA,WAAI,mCAAqC,CAAC,cAC1CN,KAAA,CAAAM,aAAA,UAAK,CACD,CAAC,CAEX,CAEA,cAAe,CAAAD,SAAS","ignoreList":[]},"metadata":{},"sourceType":"module"}